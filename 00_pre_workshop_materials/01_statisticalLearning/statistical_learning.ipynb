{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic statistical techniques and unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Clustering<br>\n",
    "    1.1 K-Means clustering<br>\n",
    "    1.2 Hierarchical clustering<br>\n",
    "    1.3 Self-organizing maps<br>\n",
    "    \n",
    "2. Dimensionality reduction<br>\n",
    "    2.1 PCA for data visualization<br>\n",
    "    2.2 PCA for data pre-processing<br>\n",
    "    2.3 PCA for simpler supervised-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Python packages\n",
    "Fortunately a lot of common data science tools are freely available in the python ecosystem (look at PyPi for details). Scikit-learn is a great place to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "from sklearn import datasets # Cancer dataset\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler # Data preprocessors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.decomposition import PCA # Principal components\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering # Clustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.ensemble import RandomForestClassifier # RFR\n",
    "\n",
    "seed=42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google colab compatibility\n",
    "Clone the repo to get the DIY data (to be used later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/argonne-lcf/sdl_ai_workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca(X, components=[1, 2], figsize=(8, 6),\n",
    "             color_vector=None, scale=False, title=None):\n",
    "    \"\"\"\n",
    "    Apply PCA to input X.\n",
    "    Args:\n",
    "        color_vector : each element corresponds to a row in X. Unique elements are colored with a different color.\n",
    "\n",
    "    Returns:\n",
    "        pca : object of sklearn.decomposition.PCA()\n",
    "        x_pca : pca matrix\n",
    "        fig : PCA plot figure handle\n",
    "    \"\"\"\n",
    "    if color_vector is not None:\n",
    "        assert len(X) == len(color_vector), 'len(df) and len(color_vector) must be the same size.'\n",
    "        n_colors = len(np.unique(color_vector))\n",
    "        colors = iter(cm.rainbow(np.linspace(0, 1, n_colors)))\n",
    "\n",
    "    X = pd.DataFrame(X)\n",
    "\n",
    "    # PCA\n",
    "    if scale:\n",
    "        xx = StandardScaler().fit_transform(X.values)\n",
    "    else:\n",
    "        xx = X.values\n",
    "\n",
    "    n_components = max(components)\n",
    "    pca = PCA(n_components=n_components)\n",
    "    x_pca = pca.fit_transform(xx)\n",
    "    pc0 = components[0] - 1\n",
    "    pc1 = components[1] - 1\n",
    "\n",
    "    # Start plotting\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    alpha = 0.7\n",
    "\n",
    "    if color_vector is not None:\n",
    "        for color in np.unique(color_vector):\n",
    "            idx = color_vector == color\n",
    "            c = next(colors)\n",
    "            ax.scatter(x_pca[idx, pc0], x_pca[idx, pc1], alpha=alpha,\n",
    "                       marker='o', edgecolor=c, color=c,\n",
    "                       label=f'{color}')\n",
    "    else:\n",
    "        ax.scatter(x_pca[:, pc0], x_pca[:, pc1], alpha=alpha,\n",
    "                   marker='s', edgecolors=None, color='b')\n",
    "\n",
    "    ax.set_xlabel('PC' + str(components[0]))\n",
    "    ax.set_ylabel('PC' + str(components[1]))\n",
    "    ax.legend(loc='lower left', bbox_to_anchor=(1.01, 0.0), ncol=1, borderaxespad=0, frameon=True)\n",
    "    plt.grid(True)\n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "\n",
    "    print('Explained variance by PCA components [{}, {}]: [{:.5f}, {:.5f}]'.format(\n",
    "        components[0], components[1],\n",
    "        pca.explained_variance_ratio_[pc0],\n",
    "        pca.explained_variance_ratio_[pc1]))\n",
    "\n",
    "    return pca, x_pca\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cancer_data():\n",
    "    \"\"\" Return cancer dataset (unscaled).\n",
    "    Returns:\n",
    "        X, Y\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    from sklearn import datasets\n",
    "    data = datasets.load_breast_cancer()\n",
    "\n",
    "    # Get features and target\n",
    "    X = pd.DataFrame(data['data'], columns=data['feature_names'])\n",
    "    X = X[sorted(X.columns)]\n",
    "    Y = data['target']\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_kmeans_obj(X_sc, tot_clusters=10):\n",
    "    opt_obj_vec = []\n",
    "    for k in range(1, tot_clusters):\n",
    "        model = KMeans(n_clusters=k)  \n",
    "        model.fit(X_sc)\n",
    "        opt_obj_vec.append(model.inertia_/X_sc.shape[0])\n",
    "        \n",
    "    # Plot\n",
    "    k = np.arange(len(opt_obj_vec)) + 1\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(k, opt_obj_vec, '--o')\n",
    "    plt.xlabel('Number of clusters (k)', fontsize=14)\n",
    "    plt.ylabel('Inertia', fontsize=14)\n",
    "    plt.grid(True)\n",
    "        \n",
    "    return opt_obj_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hists(k_means_bins, y_bins, x_labels = ['Malignant', 'Benign']):\n",
    "    \"\"\" Specific function to plot histograms from bins.\n",
    "    matplotlib.org/gallery/lines_bars_and_markers/barchart.html#sphx-glr-gallery-lines-bars-and-markers-barchart-py\n",
    "    \"\"\"\n",
    "    x = np.arange(len(x_labels))  # the label locations\n",
    "    width = 0.35  # the width of the bars\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    rects1 = ax.bar(x - width/2, y_bins, width, label='True label')\n",
    "    rects2 = ax.bar(x + width/2, k_means_bins, width, label='K-means')\n",
    "\n",
    "    # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "    ax.set_ylabel('Total count')\n",
    "    ax.set_title('Histogram')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(x_labels)\n",
    "    ax.set_ylim(0, 450)\n",
    "\n",
    "    def autolabel(rects):\n",
    "        \"\"\" Attach a text label above each bar in *rects*, displaying its height. \"\"\"\n",
    "        for rect in rects:\n",
    "            height = rect.get_height()\n",
    "            ax.annotate('{}'.format(height),\n",
    "                        xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                        xytext=(0, 3),  # 3 points vertical offset\n",
    "                        textcoords=\"offset points\",\n",
    "                        ha='center', va='bottom')\n",
    "    \n",
    "    ax.legend(loc='best')\n",
    "\n",
    "    autolabel(rects1)\n",
    "    autolabel(rects2)\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Related to SOM - finding best matching unit\n",
    "def find_bmu(t, net, n):\n",
    "    \"\"\"\n",
    "        Find the best matching unit for a given vector, t, in the SOM\n",
    "        Returns: a (bmu, bmu_idx) tuple where bmu is the high-dimensional BMU\n",
    "                 and bmu_idx is the index of this vector in the SOM\n",
    "    \"\"\"\n",
    "    bmu_idx = np.array([0, 0])\n",
    "    # set the initial minimum distance to a huge number\n",
    "    min_dist = np.iinfo(np.int).max\n",
    "    # calculate the high-dimensional distance between each neuron and the input\n",
    "    for x in range(net.shape[0]):\n",
    "        for y in range(net.shape[1]):\n",
    "            w = net[x, y, :].reshape(n, 1)\n",
    "            # don't bother with actual Euclidean distance, to avoid expensive sqrt operation\n",
    "            sq_dist = np.sum((w - t) ** 2)\n",
    "            if sq_dist < min_dist:\n",
    "                min_dist = sq_dist\n",
    "                bmu_idx = np.array([x, y])\n",
    "    # get vector corresponding to bmu_idx\n",
    "    bmu = net[bmu_idx[0], bmu_idx[1], :].reshape(n, 1)\n",
    "    # return the (bmu, bmu_idx) tuple\n",
    "    return (bmu, bmu_idx)\n",
    "\n",
    "# Decaying radius of influence \n",
    "def decay_radius(initial_radius, i, time_constant):\n",
    "    return initial_radius * np.exp(-i / time_constant)\n",
    "\n",
    "# Decaying learning rate\n",
    "def decay_learning_rate(initial_learning_rate, i, n_iterations):\n",
    "    return initial_learning_rate * np.exp(-i / n_iterations)\n",
    "\n",
    "# Influence in 2D space\n",
    "def calculate_influence(distance, radius):\n",
    "    return np.exp(-distance / (2* (radius**2)))\n",
    "\n",
    "# Update weights\n",
    "def update_weights(net,bmu_idx,r,l):\n",
    "    wlen = net.shape[2]\n",
    "    for x in range(net.shape[0]):\n",
    "        for y in range(net.shape[1]):\n",
    "            w = net[x, y, :].reshape(wlen, 1)\n",
    "            # get the 2-D distance (again, not the actual Euclidean distance)\n",
    "            w_dist = np.sum((np.array([x, y]) - bmu_idx) ** 2)\n",
    "            # if the distance is within the current neighbourhood radius\n",
    "            if w_dist <= r**2:\n",
    "                # calculate the degree of influence (based on the 2-D distance)\n",
    "                influence = calculate_influence(w_dist, r)\n",
    "                # now update the neuron's weight using the formula:\n",
    "                # new w = old w + (learning rate * influence * delta)\n",
    "                # where delta = input vector (t) - old w\n",
    "                new_w = w + (l * influence * (t - w))\n",
    "                # commit the new weight\n",
    "                net[x, y, :] = new_w.reshape(1, wlen)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Clustering\n",
    "Our objective is to segregate the dataset into discrete components representing low-dimensional structures that are not immediately visible. We will demonstrate this, first, on a toy dataset that we _design_ to have a lower inherent dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy dataset (2-D blobs)\n",
    "Define 3 blobs of data on a 2D plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 300\n",
    "n_features = 2\n",
    "n_clusters = 3\n",
    "\n",
    "X, y = datasets.make_blobs(n_samples = n_samples,\n",
    "                           n_features = n_features,\n",
    "                           centers = n_clusters,\n",
    "                           random_state = seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features\n",
    "X_sc = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 7))\n",
    "plt.scatter(X_sc[:, 0], X_sc[:, 1], edgecolor=None, alpha=0.5, c='b');\n",
    "\n",
    "plt.title('2-D dataset', fontsize=14)\n",
    "plt.xlabel('X1', fontsize=14)\n",
    "plt.ylabel('X2', fontsize=14)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 K-means Clustering\n",
    "\n",
    "K-means clustering aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells.\n",
    "\n",
    "<img src=\"image.png\">\n",
    "\n",
    "Figure courtesy Chen and Lai, 2018, Phys. Rev. E 97(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kmeans++ - Arthur and Vassilvitskii (2007) \n",
    "The exact algorithm is as follows (wikipedia):\n",
    "\n",
    "1. Choose one center uniformly at random from among the data points.\n",
    "2. For each data point x, compute D(x), the distance between x and the nearest center that has already been chosen.\n",
    "3. Choose one new data point at random as a new center, using a weighted probability distribution where a point x is chosen with probability proportional to D(x)2.\n",
    "4. Repeat Steps 2 and 3 until k centers have been chosen.\n",
    "\n",
    "Define k-means object and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=n_clusters, init='k-means++', n_init=10, max_iter=300, random_state=seed, n_jobs=4)\n",
    "model.fit(X_sc);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`labels_` returns the labels for each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = model.labels_\n",
    "print(np.bincount(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cluster_centers_` returns the coordinates of the centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = model.cluster_centers_\n",
    "print(centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot and color with labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 7))\n",
    "plt.scatter(X_sc[:, 0], X_sc[:, 1], edgecolor=None, alpha=0.5, c=labels);\n",
    "\n",
    "plt.title('2-D dataset (clustered with k-means)', fontsize=14)\n",
    "plt.xlabel('X1', fontsize=14)\n",
    "plt.ylabel('X2', fontsize=14)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`inertia_` returns sum of squared distances of samples to their closest clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.inertia_/X.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many *k* clusters to choose?\n",
    "Inertia proves useful in assessing this - fit data with different _k_ and assess inertia. If inertia has _flatlined_ you've probably reached an indicative value of _k_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kmeans_obj(X_sc, tot_clusters=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breast cancer dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.load_breast_cancer()\n",
    "print(type(data))  # dictionary-like object\n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Details of the data can be found here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(data['DESCR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get features and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(data['data'], columns=data['feature_names'])\n",
    "X = X[sorted(X.columns)]  # sort column by name\n",
    "\n",
    "Y = pd.Series(data['target'])\n",
    "Y = pd.Series(['Malignant' if v==0 else 'Benign' for v in Y.values]).astype('category') # convert to categorical\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets peek at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display( X.iloc[:3,:7] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_sc = pd.DataFrame(scaler.transform(X), columns=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define K-means model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2)\n",
    "kmeans.fit(X_sc);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot histogram of true class labels (malignant/benign) and cluster labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means_bins = sorted(np.bincount(kmeans.labels_))\n",
    "y_bins = sorted(Y.value_counts().values)\n",
    "x_labels = ['Malignant', 'Benign']\n",
    "\n",
    "plot_hists(k_means_bins, y_bins, x_labels=['Malignant', 'Benign'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kmeans_obj(X_sc, tot_clusters=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beware\n",
    "In this case - we knew that 2 clusters (benign or malignant) were appropriate for our model. YMMV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Hierarchical Clustering (agglomerative)\n",
    "This technique uses a bottom-up approach were data points are _merged_ according to a distance metric. The result is a plot called a dendrogram which reveals underlying structure in your data as well.\n",
    "\n",
    "Get a random subset of cancer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 30\n",
    "idx = np.random.choice(np.arange(X_sc.shape[0]), size=samples)\n",
    "\n",
    "X_sc_sub = X_sc.iloc[idx, :].values\n",
    "Y_sub = Y[idx].values\n",
    "\n",
    "print(X_sc_sub.shape)\n",
    "print(Y_sub.shape)\n",
    "print(pd.Series(Y_sub).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define hierarchical clustering model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AgglomerativeClustering(affinity='euclidean', linkage='ward', compute_full_tree='auto')\n",
    "model.fit(X_sc_sub);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linkage\n",
    "Scikit-learn offers the following merging options\n",
    "1. Maximum or complete linkage minimizes the maximum distance between observations of pairs of clusters.\n",
    "2. Average linkage minimizes the average of the distances between all observations of pairs of clusters.\n",
    "3. Single linkage minimizes the distance between the closest observations of pairs of clusters.\n",
    "4. Ward minimizes the sum of squared differences within all clusters. It is a variance-minimizing approach and in this sense is similar to the k-means objective function but tackled with an agglomerative hierarchical approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll actually use a different module to plot the dendrogram\n",
    "linkage_matrix = linkage(X_sc_sub, 'ward')\n",
    "\n",
    "figure = plt.figure(figsize=(15, 5))\n",
    "dendrogram(\n",
    "    linkage_matrix,\n",
    "    labels=Y_sub,\n",
    "    leaf_rotation=60,\n",
    "    above_threshold_color='y',\n",
    "    color_threshold=12.5);\n",
    "\n",
    "plt.title('Hierarchical Clustering Dendrogram', fontsize=14)\n",
    "plt.xlabel('Sample', fontsize=14)\n",
    "plt.ylabel('Distance', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Self-organizing maps\n",
    "\n",
    "Some of the concepts date back further, but SOMs were proposed and became widespread in the 1980s, by a Finnish professor named Teuvo Kohonen and are also called 'Kohonen maps'.\n",
    "\n",
    "The idea behind a SOM is that you’re mapping high-dimensional vectors onto a smaller dimensional (typically 2D) space. Vectors that are close in the high-dimensional space also end up being mapped to nodes that are close in 2D space thus preserving the \"topology\" of the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating \"Color\" data and normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = np.random.randint(0, 255, (200, 3))\n",
    "data = StandardScaler().fit_transform(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining SOM \n",
    "Defining network size, number of iterations and learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_dimensions = np.array([5, 5])\n",
    "n_iterations = 2000\n",
    "init_learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establish size variables based on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = data.shape[0]\n",
    "n = data.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight matrix (i.e. the SOM) needs to be one n-dimensional vector for each neuron in the SOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = np.random.random((network_dimensions[0], network_dimensions[1], n)) # 25 neurons each with a 3D vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial neighbourhood radius and decay parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_radius = max(network_dimensions[0], network_dimensions[1]) / 2\n",
    "time_constant = n_iterations / np.log(init_radius)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial state of SOM color network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(net, interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training SOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in range(n_iterations):\n",
    "    # select a training example at random - shape of 1x3\n",
    "    t = data[np.random.randint(0, m),:].reshape(np.array([n, 1]))\n",
    "    # find its Best Matching Unit\n",
    "    bmu, bmu_idx = find_bmu(t, net, n) # Gives the row, column of the best neuron\n",
    "    # decay the SOM parameters\n",
    "    r = decay_radius(init_radius, iteration, time_constant)\n",
    "    l = decay_learning_rate(init_learning_rate, iteration, n_iterations)\n",
    "    # Update SOM weights\n",
    "    net = update_weights(net,bmu_idx,r,l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of trained colormap SOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(net, interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.5 SOM on Cancer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = datasets.load_breast_cancer()\n",
    "# Get features and target\n",
    "X = pd.DataFrame(data['data'], columns=data['feature_names'])\n",
    "X = X[sorted(X.columns)].to_numpy()  # sort column by name\n",
    "Y = pd.Series(data['target']).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_dimensions = np.array([10, 10])\n",
    "n_iterations = 4000\n",
    "init_learning_rate = 0.01\n",
    "# establish size variables based on data\n",
    "m = X.shape[0]\n",
    "n = X.shape[1]\n",
    "\n",
    "# weight matrix (i.e. the SOM) needs to be one n-dimensional vector for each neuron in the SOM\n",
    "net = np.random.random((network_dimensions[0], network_dimensions[1], n))\n",
    "\n",
    "# initial neighbourhood radius\n",
    "init_radius = max(network_dimensions[0], network_dimensions[1]) / 2\n",
    "# radius decay parameter\n",
    "time_constant = n_iterations / np.log(init_radius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_vis = np.zeros(shape=(np.shape(net)[0],np.shape(net)[1],3),dtype='double') # Array for SOM color map visualization\n",
    "\n",
    "for sample in range(m):\n",
    "    t = X[sample,:].reshape(np.array([n, 1]))\n",
    "    # find its Best Matching Unit\n",
    "    bmu, bmu_idx = find_bmu(t, net, n)\n",
    "    net_vis[bmu_idx[0],bmu_idx[1],0] = Y[sample]\n",
    "    \n",
    "plt.imshow(net_vis)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training SOM on Cancer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in range(n_iterations):\n",
    "    # select a training example at random - shape of 1x3\n",
    "    t = X[np.random.randint(0, m),:].reshape(np.array([n, 1]))\n",
    "    # find its Best Matching Unit\n",
    "    bmu, bmu_idx = find_bmu(t, net, n)\n",
    "    # decay the SOM parameters\n",
    "    r = decay_radius(init_radius, iteration, time_constant)\n",
    "    l = decay_learning_rate(init_learning_rate, iteration, n_iterations)\n",
    "    # Update SOM weights\n",
    "    net = update_weights(net,bmu_idx,r,l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of trained SOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_vis = np.zeros(shape=(np.shape(net)[0],np.shape(net)[1],3),dtype='double') # Array for SOM color map visualization\n",
    "\n",
    "for sample in range(m):\n",
    "    t = X[sample,:].reshape(np.array([n, 1]))\n",
    "    # find its Best Matching Unit\n",
    "    bmu, bmu_idx = find_bmu(t, net, n)\n",
    "    net_vis[bmu_idx[0],bmu_idx[1],0] = Y[sample] # Red if benign\n",
    "    \n",
    "plt.imshow(net_vis)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dimensionality Reduction\n",
    "Similar to k-means, PCA may also be used to reveal a lower dimensional representation of the data. This can prove to be useful since it is easier to visualize the data by _projecting_ on certain principal components. \n",
    "\n",
    "PCA finds a linear subspace that is $L_2$-optimal - and the basis vectors of this linear subspace capture the variance of the data incrementally. This is excellent for removing redundant dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "X, Y = load_cancer_data()\n",
    "# Scale data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_sc = pd.DataFrame( scaler.transform(X), columns=X.columns )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. PCA for data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find pairs of features with highest correlation\n",
    "corr = X_sc.corr('pearson').values\n",
    "np.fill_diagonal(corr, val=0) # in-place operation\n",
    "corr = pd.DataFrame(corr, index=X_sc.columns, columns=X_sc.columns)\n",
    "\n",
    "corr_th = 0.99\n",
    "x_ids, y_ids = np.where(corr > corr_th) # find indices that satisfy the condition\n",
    "    \n",
    "display(corr.iloc[x_ids, y_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and plot two features with height correlation\n",
    "x_2d = X_sc[['mean perimeter', 'mean radius']]\n",
    "\n",
    "plt.figure(figsize=(9,7))\n",
    "\n",
    "# plt.scatter(xtr['mean perimeter'], xtr['mean radius'], alpha=0.7, marker='o');\n",
    "plt.scatter(x_2d.loc[Y==1, 'mean perimeter'],\n",
    "            x_2d.loc[Y==1, 'mean radius'],\n",
    "            alpha=0.5, marker='o', c='b', edgecolor=None, label='Benign');\n",
    "\n",
    "plt.scatter(x_2d.loc[Y==0, 'mean perimeter'],\n",
    "            x_2d.loc[Y==0, 'mean radius'],\n",
    "            alpha=0.5, marker='x', c='r', edgecolor=None, label='Malignant');\n",
    "\n",
    "plt.xlabel('Mean perimeter', fontsize=14)\n",
    "plt.ylabel('Mean radius', fontsize=14)\n",
    "plt.legend(fontsize=14)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA with two highly correlated features\n",
    "pca = PCA(n_components=2)\n",
    "x_pca = pca.fit_transform(x_2d) # compute PCA and return the result\n",
    "\n",
    "# Print the proportion of explained variances\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PCA\n",
    "pca, _ = plot_pca( X=x_2d,\n",
    "                   components=[1, 2],\n",
    "                   color_vector=Y, title='X: only two features', figsize=(9,7) );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PCA of the full dataset\n",
    "pca, _ = plot_pca( X=X_sc,\n",
    "                   components=[1, 2],\n",
    "                   color_vector=Y, title='X: full dataset', figsize=(9,7) );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. PCA for data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train/test\n",
    "xtr, xte, ytr, yte = train_test_split(X_sc, Y, test_size=0.2)\n",
    "\n",
    "xtr.reset_index(drop=True, inplace=True)\n",
    "xte.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print('X train shape:', xtr.shape)\n",
    "print('X test shape: ', xte.shape)\n",
    "print('Y train shape:', ytr.shape)\n",
    "print('Y test shape: ', yte.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA xtr and xte\n",
    "pca = PCA(n_components=xtr.shape[1])\n",
    "xtr_pca = pca.fit_transform(xtr)  # PCA of the training set\n",
    "xte_pca = pca.transform(xte)      # Apply the PCA transformation to the test set\n",
    "\n",
    "print('X train PCA shape:', xtr_pca.shape)\n",
    "print('X test PCA shape: ', xte_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot explained variance PCA\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(pca.explained_variance_ratio_, '--o')\n",
    "plt.xlabel('Principal component', fontsize=14)\n",
    "plt.ylabel('Variance explained (ratio)', fontsize=14)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features are highly redundant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_exp_var(pca, k):\n",
    "    \"\"\" Print the total variance of the k principal components. \"\"\"\n",
    "    print (np.sum(pca.explained_variance_ratio_[:k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_exp_var(pca, k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 PCA for improved supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Random Forest Classifier\n",
    "rf = RandomForestClassifier(n_estimators=100, oob_score=True, min_samples_split=5)\n",
    "# Train Classifier\n",
    "rf.fit(xtr, ytr);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test set (inference)\n",
    "y_pred = rf.predict(xte)\n",
    "f1 = f1_score(yte, y_pred)\n",
    "print('F1 score:', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier with *K* PCA components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many PCA components should we retain?\n",
    "- You decide that you want to keep cetain amount of explained variance (e.g., 0.9, 0.95, 0.99).\n",
    "- Look for the \"elbow\" in the explained variance plot.\n",
    "- Use *k* as a hyperparameter (choose *k* that improves the generalization of your supervised learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chooose how many PCA components to use\n",
    "k = 17\n",
    "pca_exp_var(pca, k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data for k PCA components\n",
    "xtr_pca_sub = xtr_pca[:, :k]  # PCA of the training set\n",
    "xte_pca_sub = xte_pca[:, :k]  # PCA of the test set\n",
    "\n",
    "print(xtr_pca_sub.shape)\n",
    "print(xte_pca_sub.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Random Forest Classifier\n",
    "rf_pca = RandomForestClassifier(n_estimators=100, oob_score=True, min_samples_split=5)\n",
    "\n",
    "# Train Classifier\n",
    "rf_pca.fit(xtr_pca_sub, ytr);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf_pca.predict(xte_pca_sub)\n",
    "f1 = f1_score(yte, y_pred)\n",
    "print('F1 score:', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra DIY stuff - Pilot1 cancer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('./rna.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract rna and meta\n",
    "rna_cols = [c for c in df.columns if 'GE_' in c]\n",
    "rna = df[rna_cols]\n",
    "meta = df.drop(columns=['Unnamed: 0'] + rna_cols)\n",
    "\n",
    "print(rna.shape)\n",
    "print(meta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show features (gene expression)\n",
    "display(rna.iloc[:3, :7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show meta data\n",
    "display(meta[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'csite'\n",
    "meta[col].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, rna_pca = plot_pca(X=rna, components=[1,2], color_vector=meta[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=n_clusters, init='k-means++', random_state=seed, n_jobs=4)\n",
    "model.fit(rna)\n",
    "\n",
    "labels = model.labels_\n",
    "print('Labels bincount:', np.bincount(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkage_matrix = linkage(rna, 'ward')\n",
    "figure = plt.figure(figsize=(15, 5))\n",
    "dendrogram(\n",
    "    linkage_matrix,\n",
    "    color_threshold=40,\n",
    "    labels=meta[col].values,\n",
    "    leaf_rotation=60,\n",
    "    show_contracted=True,\n",
    "    above_threshold_color='y'\n",
    ")\n",
    "plt.title('Hierarchical Clustering Dendrogram (Ward)')\n",
    "plt.xlabel('sample index')\n",
    "plt.ylabel('distance')\n",
    "plt.tight_layout()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical clustering makes it easier to find sub-populations in data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. lagunita.stanford.edu/courses/HumanitiesSciences/StatLearning/Winter2016/course/\n",
    "2. www.coursera.org/learn/ml-clustering-and-retrieval/\n",
    "3. www.coursera.org/learn/machine-learning/home/week/8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minigap",
   "language": "python",
   "name": "minigap"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
